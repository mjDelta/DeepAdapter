{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2d9f710-01cb-4972-b9f9-273e95285687",
   "metadata": {},
   "source": [
    "## Tutorial of DeepAdapter\n",
    "### A self-adaptive and versatile tool for eliminating multiple undesirable variations from transcriptome\n",
    "In this notebook, you will learn how to re-train DeepAdapter with the example dataset.\n",
    "\n",
    "## 1. Installation and requirements\n",
    "### 1.1. Installation\n",
    "To run locally, please open a terminal window and download the code with:\n",
    "```sh\n",
    "$ # create a new conda environment\n",
    "$ conda create -n DA python=3.9\n",
    "$ # activate environment\n",
    "$ conda activate DA\n",
    "$ # Install dependencies\n",
    "$ pip install deepadapter\n",
    "$ # Launch jupyter notebook\n",
    "$ jupyter notebook\n",
    "```\n",
    "### 1.2. Download datasets\n",
    "Please download the open datasets in [Zenodo](https://zenodo.org/records/10494751).\n",
    "These datasets are collected from literatures to demonstrate multiple unwanted variations, including:\n",
    "* batch datasets: LINCS-DToxS ([van Hasselt et al. Nature Communications, 2020](https://www.nature.com/articles/s41467-020-18396-7)) and Quartet project ([Yu, Y. et al. Nature Biotechnology, 2023](https://www.nature.com/articles/s41587-023-01867-9)).\n",
    "* platform datasets: profiles from microarray ([Iorio, F. et al. Cell, 2016](https://www.cell.com/cell/pdf/S0092-8674(16)30746-2.pdf)) and RNA-seq ([Ghandi, M. et al. Nature, 2019](https://www.nature.com/articles/s41586-019-1186-3)).\n",
    "* purity datasets: profiles from cancer cell lines ([Ghandi, M. et al. Nature, 2019](https://www.nature.com/articles/s41586-019-1186-3)) and tissues ([Weinstein, J.N. et al. Nature genetics, 2013](https://www.nature.com/articles/ng.2764)).\n",
    "\n",
    "After downloading, place the datasets in the `data/` directory located in the same hierarchy as this tutorial.\n",
    "* batch datasets: `data/batch_data/`\n",
    "* platform datasets: `data/platform_data/`\n",
    "* purity datasets: `data/purity_data/`\n",
    "  \n",
    "**Putting datasets in the right directory is important for loading the example datasets successfully.**\n",
    "\n",
    "To execute a \"cell\", please press Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c179c-882b-4e1e-aba7-0594a6647bc5",
   "metadata": {},
   "source": [
    "## 2. Load the datasets and preprocess\n",
    "### 2.1. load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbc07082-e02d-431f-9fba-b5b78c85fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengjizhang/miniconda3/envs/DA/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from deepadapter.utils import data_utils as DT\n",
    "from deepadapter.finetune_utils as FTUT\n",
    "from deepadapter.params import dl_finetune_params as DLPARAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56913165-41eb-4ce8-a8dc-496d7dc71f07",
   "metadata": {},
   "source": [
    "### 2.2. Load the demonstrated datasets\n",
    "We ultilize Batch-LINCS for demonstration. To load datasets of platform and purity variations, please download them in Zenodo (https://zenodo.org/records/10494751).\n",
    "  * In the tutorial, we have **data** for gene expression, **batches** for unwanted variations, and **donors** for biological signals.\n",
    "  * If you want to fine-tune with your own data, please refer to `DeepAdapter-YourOwnData-Finetune.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c4e81d-f681-43bb-b299-e7953ff7e2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load LINCS LDS 1593, size of (709, 10112)\n"
     ]
    }
   ],
   "source": [
    "loadTransData = DT.LoadTransData()\n",
    "data, batches, wells, donors, infos, test_infos = loadTransData.load_lincs_lds1593()\n",
    "ids = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0e84f-e5c6-4964-98ff-0e987be7e027",
   "metadata": {},
   "source": [
    "### 2.3 Load the genes used in pre-trained model\n",
    "Before fine-tuning, make sure that the loaded genes are the genes used in the pre-trained model. The pre-trained models can be found in the folder of `models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69958660-8d77-4785-8514-fe4063b9c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = \"models/pretrained_LINCS_Quartet/\"\n",
    "pretrain_genes = pd.read_csv(os.path.join(load_dir, \"gene.csv\"))[\"gene\"]\n",
    "try:\n",
    "\tdata = data[pretrain_genes]\n",
    "except Exception as e:\n",
    "    raise(\"Inconsistent gene set between this dataset and pretrained dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845814c-c265-4bf1-b662-a685d52933a9",
   "metadata": {},
   "source": [
    "### 2.3. Preprocess the transcriptomic data\n",
    "The gene expression profiles are preprocessed by sample normalization, gene ranking, and log normalization. Let $S_i = \\sum_l x_{i l}$ denote the sum over all genes. In sample normalization, we divide $S_i$ for every sample and multiply a constant 10000 ([Xiaokang Yu et al. Nature communications, 2023](https://www.nature.com/articles/s41467-023-36635-5)):\n",
    "$$x_{i l} = \\frac{x_{i l}}{S_i} 10^4.$$\n",
    "Then, we sort genes by their expression levels and perform the log transformation $x_{i l} = \\log {(x_{i l} + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d387d965-c88f-4aed-95cd-b6dbf2d0017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepTransData = DT.PrepTransData()\n",
    "raw_df = prepTransData.sample_norm(data)\n",
    "input_arr = prepTransData.sample_log(raw_df)\n",
    "bat2label, label2bat, unwanted_labels, unwanted_onehot = prepTransData.label2onehot(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed7bdaf-271f-4754-af3d-fe7bb4d93f25",
   "metadata": {},
   "source": [
    "## 3. Fine-tune DeepAdapter\n",
    "### 3.1. Adjust DeepAdapter's parameters\n",
    "The parameters for DeepAdapter are as follows (**Note: you can revise parameter directly in `net_args`, e.g., `net_args.batch_size = 512`.**):\n",
    "* **epochs**: the fine-tune epochs of DeepAdapter, default = $5000$\n",
    "* **ae_epochs**: the warmup epochs of autoencoder in DeepAdapter, default = $400$\n",
    "* **batch_epochs**: the warmup epochs of discriminator in DeepAdapter, default = $50$\n",
    "* **batch_size**: the batch size of dataloader, default = $256$\n",
    "* **hidden_dim**: the hidden units of autoencoder in DeepAdapter, default = $256$\n",
    "* **z_dim**: the latent units of autoencoder in DeepAdapter, default = $128$\n",
    "* **drop**: the dropout rate of DeepAdapter, default = $0.3$\n",
    "* **lr_lower_ae**: the lower learning rate of autoencoder in DeepAdapter, default = $1e-5$\n",
    "* **lr_upper_ae**: the upper learning rate of autoencoder in DeepAdapter, default = $5e-4$\n",
    "* **lr_lower_batch**: the lower learning rate of discriminator in DeepAdapter, default = $1e-5$\n",
    "* **lr_upper_batch**: the upper learning rate of discriminator in DeepAdapter, default = $5e-4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a84343-cb36-4a4d-93c6-7637eef175db",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_args = DLPARAM.load_dl_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d92d21-32e7-438c-94a2-606045a2ac21",
   "metadata": {},
   "source": [
    "### 3.2. Split dataset\n",
    "* For the tutorial, we extract the biosamples across all batches as the test set; then split the rest into training and validation set randomly.</br>\n",
    "That means the training data seen by DeepAdapter doesn't disperse across all unwanted variations while the testing data does.</br>\n",
    "Acutally, this split method could increase the training difficulty.\n",
    "* For your own dataset, you can split the dataset randomly using the function `DT.data_split_random`.</br>\n",
    "Please refer to `DeepAdapter-YourOwnData-Tutorial.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb151d-5a9f-414a-a25b-4dca43a7b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_labels_hot, \\\n",
    "    val_data, val_labels, val_labels_hot, \\\n",
    "    test_data, test_labels, test_labels_hot, \\\n",
    "    train_ids, val_ids, test_ids, \\\n",
    "    tot_train_val_idxs, tot_train_idxs, tot_val_idxs, tot_test_idxs = DT.data_split_lds1593(input_arr, unwanted_labels, unwanted_onehot, ids, infos, test_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32181bab-39f6-49c5-8b11-65a00562fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bios, val_bios, test_bios = donors[tot_train_idxs], donors[tot_val_idxs], donors[tot_test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4302d-0e87-40a3-9793-e10fc3e66bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_label2bat = {t:t for t in set(train_bios)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aaab98-836f-4f74-9b4a-e2af4c65fd52",
   "metadata": {},
   "source": [
    "### 3.3. Fine-tune DeepAdapter\n",
    "Given that the fine-tuned dataset encompasses a different number of batch categories (4 batches) compared to the pre-trained dataset (21 batches), we modify the last layer of the discriminatory network to classify 4 batch categories instead of 21. We unfreeze all layers and train the fine-tuned models for 5K epochs. This procedure is repeated 100 times, with performances assessed by an independent testing set of 24 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba951b1-797d-409e-ab41-e5a0e048471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_num = net_args.ft_num\n",
    "\n",
    "out_dir = f\"models/finetune_LINCS_Quartet/record_{ft_num}\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "num_platform = len(bat2label)\n",
    "data, labels, labels_hot, ids = unwanted_onehot, unwanted_labels, unwanted_onehot, np.arange(len(unwanted_onehot))\n",
    "\n",
    "FTUT.test_finetune(data, labels, labels_hot, donors, ids, label2bat, load_dir, out_dir, net_args, num_platform, finetune_num = ft_num, n_test = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
