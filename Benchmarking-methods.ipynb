{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c8eca5-c0a4-4b6d-92ce-9cec5d15e2bd",
   "metadata": {},
   "source": [
    "# Benchmarking methods comparison\n",
    "## 1. Installation and requirements\n",
    "### 1.1 Installation\n",
    "#### 1.1.1 Extra installation for Windows\n",
    "Before installing the packages, please make sure **Microsoft Visual C++ 14.0 or greater** installed. The official installation link can be found [VC++](https://visualstudio.microsoft.com/visual-cpp-build-tools/).\n",
    "#### 1.1.2 Python packages\n",
    "Before running, please ensure the packages of related methods are installed. These benchmarking methods are: **Quantile Normalization**, **Combat**, **MNN**, **Harmony**, **PRPS**, **Scanorama**, **BBKNN**, **AutoClss**, and **scVI**.\n",
    "\n",
    "These benchmarking methods rely on different running environment:\n",
    "\n",
    "A. Methods that are compatible to DA environment include **Quantile Normalization**, **Combat**, **Harmony**, **PRPS**, **Scanorama**, **BBKNN**, **AutoClss**, and **scVI**.\n",
    "\n",
    "```sh\n",
    "$ conda activate DA\n",
    "$ pip install qnorm==0.8.1 combat==0.3.3 scanorama==1.7.4 anndata bbknn==1.6.0 scanpy harmonypy scvi==0.6.8 tensorflow\n",
    "$ git clone https://github.com/datapplab/AutoClass\n",
    "$ jupyter notebook\n",
    "```\n",
    "In jupyter notebook, open `Benchmarking methods.ipynb` to run the methods that are compatible to DA environment.\n",
    "\n",
    "B. Method that is not compatible to DA environment includes **MNN**.\n",
    "\n",
    "Before running, ensure that the codes in `mnn_utils/` for loading the dataset are the same hierarchy as this tutorial. To run **MNN**, please create the environment with th following codes:\n",
    "```sh\n",
    "$ conda create -n py3.8 python=3.8\n",
    "$ conda activate py3.8\n",
    "$ pip install mnnpy==0.1.9.5 matplotlib tqdm umap-learn openpyxl scipy==1.5.4\n",
    "$ python Benchmarking-MNN.py\n",
    "```\n",
    "\n",
    "*Noting: some packages don't support separate training and testing. For these packages, we will load the whole data as the training set. In other words, these methods will be trained with more samples used by **DeepAdapter**. The comparison will be unfiar to **DeepAdapter** which uses less samples. Yet, **DeepAdapter** wins.*\n",
    "\n",
    "#### 1.1.3 Potential environmental errors and solutions for installation\n",
    "After running the codes to install packages for **MNN**, there might be some environmental errors with running the **multiprocessing** package required by **mnnpy**. If the errors appear, we would suggest you comment the codes with multiprocessing acceleration. The multiprocessing works for accelerating the calculation speed and does not affect the aligned performances.\n",
    "\n",
    "**Solution:** \n",
    "1. Open the folder that you installed with `pip install mnnpy==0.1.9.5`. It might be in `~/user_name/anaconda3/envs/py3.8/Lib/site-packages/mnnpy`.\n",
    "2. Comment the `mnn.py` in line 191 and 192 and add one-line code. The commented codes should look like:\n",
    "   ```\n",
    "   191 # with Pool(n_jobs) as p_n:\n",
    "   192 #     angle_out = p_n.map(find_subspace_job, correction_in)\n",
    "   193 angle_out = find_subspace_job(correction_in)\n",
    "   ```\n",
    "3. Open `settings.py` and replace **parallel** with **nonparallel**.\n",
    "\n",
    "\n",
    "### 1.2 Datasets\n",
    "Please download the open datasets in [Zenodo](https://zenodo.org/records/10494751).\n",
    "These datasets are collected from literatures to demonstrate multiple unwanted variations, including:\n",
    "* batch datasets: LINCS-DToxS ([van Hasselt et al. Nature Communications, 2020](https://www.nature.com/articles/s41467-020-18396-7)) and Quartet project ([Yu, Y. et al. Nature Biotechnology, 2023](https://www.nature.com/articles/s41587-023-01867-9)).\n",
    "* platform datasets: profiles from microarray ([Iorio, F. et al. Cell, 2016](https://www.cell.com/cell/pdf/S0092-8674(16)30746-2.pdf)) and RNA-seq ([Ghandi, M. et al. Nature, 2019](https://www.nature.com/articles/s41586-019-1186-3)).\n",
    "* purity datasets: profiles from cancer cell lines ([Ghandi, M. et al. Nature, 2019](https://www.nature.com/articles/s41586-019-1186-3)) and tissues ([Weinstein, J.N. et al. Nature genetics, 2013](https://www.nature.com/articles/ng.2764)).\n",
    "\n",
    "After downloading, place the datasets in the `data/` directory located in the same hierarchy as this tutorial.\n",
    "* batch datasets: `data/batch_data/`\n",
    "* platform datasets: `data/platform_data/`\n",
    "* purity datasets: `data/purity_data/`\n",
    "  \n",
    "**Putting datasets in the right directory is important for loading the example datasets successfully.**\n",
    "\n",
    "To execute a \"cell\", please press Shift+Enter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a5e29-0272-4a7b-af7b-b64c3905bc16",
   "metadata": {},
   "source": [
    "## 2. Load the datasets and preprocess\n",
    "### 2.1. load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from deepadapter.utils import data_utils as DT\n",
    "from deepadapter.utils import decomposition_utils as DPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3897d06d",
   "metadata": {},
   "source": [
    "### 2.2. Load the demonstrated datasets\n",
    "We ultilize Batch-LINCS for demonstration. To load datasets of platform and purity variations, please download them in Zenodo (https://zenodo.org/records/10494751).\n",
    "  * In the tutorial, we have **data** for gene expression, **batches** for unwanted variations, and **donors** for biological signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67befd6f-9a35-4920-88f9-3b0de124a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadTransData = DT.LoadTransData()\n",
    "data, batches, wells, donors, infos, test_infos = loadTransData.load_lincs_lds1593()\n",
    "ids = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3336042-f50b-4709-ae7e-e9f53a5d8e78",
   "metadata": {},
   "source": [
    "### 2.3. Preprocess the transcriptomic data\n",
    "The gene expression profiles are preprocessed by sample normalization, gene ranking, and log normalization. Let $S_i = \\sum_l x_{i l}$ denote the sum over all genes. In sample normalization, we divide $S_i$ for every sample and multiply a constant 10000 ([Xiaokang Yu et al. Nature communications, 2023](https://www.nature.com/articles/s41467-023-36635-5)):\n",
    "$$x_{i l} = \\frac{x_{i l}}{S_i} 10^4.$$\n",
    "Then, we sort genes by their expression levels and perform the log transformation $x_{i l} = \\log {(x_{i l} + 1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da30622",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepTransData = DT.PrepTransData()\n",
    "raw_df = prepTransData.sample_norm(data)\n",
    "raw_df, sorted_cols = prepTransData.sort_genes_sgl_df(raw_df)\n",
    "input_arr = prepTransData.sample_log(raw_df)\n",
    "bat2label, label2bat, unwanted_labels, unwanted_onehot = prepTransData.label2onehot(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed55fc2",
   "metadata": {},
   "source": [
    "## 3. Run the benchmarking methods\n",
    "### 3.1 Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038dbe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, train_labels_hot, \\\n",
    "    val_data, val_labels, val_labels_hot, \\\n",
    "    test_data, test_labels, test_labels_hot, \\\n",
    "    train_ids, val_ids, test_ids, \\\n",
    "    tot_train_val_idxs, tot_train_idxs, tot_val_idxs, tot_test_idxs = DT.data_split_lds1593(input_arr, unwanted_labels, unwanted_onehot, ids, infos, test_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd43cd",
   "metadata": {},
   "source": [
    "### 3.2 Align the datasets by benchmarking methods\n",
    "Please assign the method name for comparison by:\n",
    "\n",
    "`baseline = [Name]`, where `[Name]` can be chosen from:\n",
    "- quantile\n",
    "- combat\n",
    "- harmony\n",
    "- scanorama\n",
    "- bbknn\n",
    "- autoclass\n",
    "- scvi-orig\n",
    "- scvi-opti\n",
    "\n",
    "Noting: MNN can be run via `python Benchmarking-MNN.py` in the different environment. Find the instructions of creating this environment in **1. Installation and requirements**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95ff7e-2dbd-4ff0-938b-d3b5602541d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"scvi-optim\"\n",
    "out_dir = f\"./baselines_out/{baseline}_15K/\"\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16397625-df51-49cf-a4dc-e5d25e12a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline == \"quantile\":\n",
    "    import qnorm\n",
    "    data_df = pd.DataFrame(input_arr, columns = sorted_cols, index = ids).transpose()\n",
    "    normed_corrected = qnorm.quantile_normalize(data_df, axis=1).transpose()\n",
    "    normed_data = normed_corrected.values.squeeze()\n",
    "    labels = unwanted_labels.copy()\n",
    "elif baseline == \"combat\":\n",
    "    from combat.pycombat import pycombat\n",
    "    data_df = pd.DataFrame(input_arr, columns = sorted_cols, index = ids)\n",
    "    data_corrected = pycombat(data_df.transpose(), unwanted_labels).transpose()\n",
    "    normed_data = data_corrected.values.squeeze()\n",
    "    labels = unwanted_labels.copy()\n",
    "elif baseline == \"scanorama\":\n",
    "    import scanorama\n",
    "    import anndata as ad\n",
    "    new_labels, adatas = [], []\n",
    "    for t in sorted(set(unwanted_labels)):\n",
    "        mask = unwanted_labels == t\n",
    "        dataset = input_arr[mask]\n",
    "        adata = ad.AnnData(dataset)\n",
    "        adata.obs_names = [f\"Sample_{i:d}\" for i in range(adata.n_obs)]\n",
    "        adata.var_names = [f\"Gene_{i:d}\" for i in range(adata.n_vars)]\n",
    "        adatas.append(adata)\n",
    "        new_labels.extend([t]*sum(mask))\n",
    "    labels = np.array(new_labels)    \n",
    "    normed_corrected = scanorama.correct_scanpy(adatas, return_dimred=True)\n",
    "    normed_data = np.vstack([normed_.to_df().values for normed_ in normed_corrected])\n",
    "elif baseline == \"bbknn\":## require the cell type information\n",
    "    import bbknn\n",
    "    import scanpy as sc\n",
    "    import anndata as ad\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_components = 50\n",
    "    pcas = PCA(n_components = n_components).fit_transform(input_arr)\n",
    "    adata = ad.AnnData(input_arr)\n",
    "    adata.obs_names = [f\"Sample_{i:d}\" for i in range(adata.n_obs)]\n",
    "    adata.obs[\"batch\"] = unwanted_labels\n",
    "    adata.obsm[\"X_pca\"] = pcas\n",
    "    adata.var_names = [f\"Gene_{i:d}\" for i in range(adata.n_vars)]\n",
    "    normed_corrected = bbknn.bbknn(adata, batch_key = \"batch\", copy = True)\n",
    "    sc.tl.umap(normed_corrected)\n",
    "    normed_data = normed_corrected.obsm['X_umap']\n",
    "    labels = unwanted_labels.copy()\n",
    "elif baseline == \"harmony\":\n",
    "    from sklearn.decomposition import PCA\n",
    "    n_components = 50\n",
    "    pcas = PCA(n_components = n_components).fit_transform(input_arr)\n",
    "    adata = ad.AnnData(input_arr)\n",
    "    adata.obs_names = [f\"Sample_{i:d}\" for i in range(adata.n_obs)]\n",
    "    adata.obs[\"batch\"] = [str(t) for t in unwanted_labels]\n",
    "    adata.obsm[\"X_pca\"] = pcas\n",
    "    adata.var_names = [f\"Gene_{i:d}\" for i in range(adata.n_vars)]\n",
    "    sc.external.pp.harmony_integrate(adata, \"batch\")\n",
    "    normed_data = adata.obsm[\"X_pca_harmony\"]\n",
    "    labels = unwanted_labels.copy()\n",
    "elif \"scvi\" in baseline:\n",
    "    import anndata as ad\n",
    "    from scvi.dataset.dataset import GeneExpressionDataset\n",
    "    from scvi.inference import UnsupervisedTrainer\n",
    "    from scvi.models import VAE\n",
    "    all_dataset = GeneExpressionDataset()\n",
    "    all_dataset.populate_from_data(train_data, batch_indices = train_labels)\n",
    "    nb_genes = train_data.shape[1]\n",
    "    n_batches = len(set(train_labels))\n",
    "    if baseline == \"scvi-orig\":\n",
    "        n_epochs = 400*(20000/len(train_data))\n",
    "        vae = VAE(nb_genes, n_batch=n_batches, n_hidden=128, n_latent=30, n_layers=2, dispersion='gene')\n",
    "    elif baseline == \"scvi-optim\":\n",
    "        n_epochs = 15000\n",
    "        vae = VAE(nb_genes, n_batch=n_batches, n_hidden=256, n_latent=128, n_layers=5, dispersion='gene')    \n",
    "    trainer = UnsupervisedTrainer(vae, all_dataset, train_size=1.0)\n",
    "    trainer.train(n_epochs=int(n_epochs))\n",
    "    all_dataset = GeneExpressionDataset()\n",
    "    all_dataset.populate_from_data(input_arr, batch_indices = unwanted_labels)\n",
    "    all_full = trainer.create_posterior(trainer.model, all_dataset, indices=np.arange(len(all_dataset)))\n",
    "    normed_data, all_batch_indices, _ = all_full.sequential().get_latent()\n",
    "    labels = all_batch_indices.ravel()\n",
    "elif baseline == \"autoclass\":\n",
    "    from AutoClass.AutoClass.AutoClass import AutoClassImpute\n",
    "    res = AutoClassImpute(input_arr)\n",
    "    normed_data = res['imp']    \n",
    "    labels = unwanted_labels.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be964ae-da32-4676-bf7e-32132357ac25",
   "metadata": {},
   "source": [
    "### Visualization of aligned dataset\n",
    "- BBKNN will return the decomposed data. Thus, there is no need to perform data decomposition for BBKNN.\n",
    "- Perform decomposition for other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7fd5a-edee-4bec-b893-87c8d57903df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decom_plot(data, labels, save_path, baseline, colors, perplexity = 30, label2name = None, min_dist = 0.99, size = 20, metric = \"euclidean\", n_neighbors = 15):\n",
    "    import umap\n",
    "    import matplotlib.pyplot as plt\n",
    "    from deepadapter.utils import utils as UT\n",
    "    label_set = sorted(set(labels))\n",
    "    if baseline != \"bbknn\":\n",
    "        fitter = umap.UMAP(random_state = 42, min_dist = min_dist, metric = metric, n_neighbors = n_neighbors)\n",
    "        trans_data = fitter.fit_transform(data)\n",
    "    else:\n",
    "        trans_data = data\n",
    "    align_score = UT.alignment_score(trans_data, labels)\n",
    "    \n",
    "    fig = plt.figure(figsize = (7, 5))\n",
    "    for l, c in zip(label_set, colors):\n",
    "        mask = labels == l\n",
    "        plt.scatter(trans_data[mask][:, 0], trans_data[mask][:, 1], edgecolor = c, color = c, \n",
    "                    s = size,\n",
    "                    linewidths = 0.5, label = label2name[l], alpha = 0.8)\n",
    "    plt.xticks(fontsize = 13)\n",
    "    plt.yticks(fontsize = 13)\n",
    "    plt.title(\"Align score of {}\".format(align_score))\n",
    "    plt.savefig(save_path, bbox_inches = \"tight\")  \n",
    "    return trans_data\n",
    "    \n",
    "colors = [\"#57904B\", \"violet\",  \"#C93C2A\", \"#372A8F\"]\n",
    "trans_aligned = decom_plot(\n",
    "        normed_data, labels, os.path.join(out_dir, \"aligned.png\"), \n",
    "        baseline, colors = colors, label2name = label2bat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6734d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe1395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
